# part4

[TOC]

---

> å†™åœ¨å‰é¢ï¼š
>
> åœ¨è·Ÿç€DWå­¦ä¹ é€”ä¸­å‰å‡ èŠ‚éƒ½æ˜¯ç”¨é˜¿é‡Œäº‘æœåŠ¡å™¨ï¼Œä½†æ˜¯å¥½åƒæ˜¯æœåŠ¡å™¨é…ç½®æœ‰ç‚¹ä½Žï¼Œè¿›è¡Œä¸€ç³»åˆ—æ“ä½œåŽå°±ä¼šå´©æºƒï¼Œä½†æ˜¯æœ€è¿‘äº‹æƒ…ï¼Œä¸èƒ½è€æ˜¯åŽ»è°ƒæ•´ï¼Œæ‰€ä»¥ä½¿ç”¨WSL2åœ¨æœ¬åœ°è¿›è¡Œæ“ä½œ
>
> ![image-20240626223923707](https://s2.loli.net/2024/06/26/2kAeNTVi5qHQBIz.png)

## å°†æ¨¡åž‹æŽ¥å…¥LangChain

### æŽ¥å…¥zhipu

> è¿™é‡Œæˆ‘é‡‡ç”¨çš„æ™ºè°±çš„apiï¼Œæ‰€ä»¥æ‰ç”¨è‡ªå®šä¹‰æŽ¥å…¥ï¼Œæˆ‘ä»¬åˆ©ç”¨DWç»™çš„ä»£ç å¿«é€ŸæŽ¥å…¥

```python
from typing import Any, List, Mapping, Optional, Dict
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.language_models.llms import LLM
from zhipuai import ZhipuAI

# ç»§æ‰¿è‡ª langchain_core.language_models.llms.LLM
class ZhipuAILLM(LLM):
    # é»˜è®¤é€‰ç”¨ glm-4 æ¨¡åž‹
    model: str = "glm-4"
    # æ¸©åº¦ç³»æ•°
    temperature: float = 0.1
    # API_Key
    api_key: str = None
    
    def _call(self, prompt : str, stop: Optional[List[str]] = None,
                run_manager: Optional[CallbackManagerForLLMRun] = None,
                **kwargs: Any):
        
        def gen_glm_params(prompt):
            '''
            æž„é€  GLM æ¨¡åž‹è¯·æ±‚å‚æ•° messages

            è¯·æ±‚å‚æ•°ï¼š
                prompt: å¯¹åº”çš„ç”¨æˆ·æç¤ºè¯
            '''
            messages = [{"role": "user", "content": prompt}]
            return messages
        
        client = ZhipuAI(
            api_key=self.api_key
        )
     
 		messages = gen_glm_params(prompt)
        response = client.chat.completions.create(
            model = self.model,
            messages = messages,
            temperature = self.temperature
        )

        if len(response.choices) > 0:
            return response.choices[0].message.content
        return "generate answer error"


    # é¦–å…ˆå®šä¹‰ä¸€ä¸ªè¿”å›žé»˜è®¤å‚æ•°çš„æ–¹æ³•
    @property
    def _default_params(self) -> Dict[str, Any]:
        """èŽ·å–è°ƒç”¨Ennie APIçš„é»˜è®¤å‚æ•°ã€‚"""
        normal_params = {
            "temperature": self.temperature,
            }
        # print(type(self.model_kwargs))
        return {**normal_params}

    @property
    def _llm_type(self) -> str:
        return "Zhipu"

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        return {**{"model": self.model}, **self._default_params}
```

å¼€å§‹æŽ¥å…¥æ“ä½œ

```python
from zhipuai_llm import ZhipuAILLM
```

```python
zhipuai_model = ZhipuAILLM(model="chatglm_std", temperature=0, api_key=api_key)
#å¯¹modelè¿›è¡Œè®¾ç½®ï¼Œä½¿ç”¨chatglm_stdé»˜è®¤æ¨¡åž‹

zhipuai_model("ä½ å¥½ï¼Œè¯·ä½ è‡ªæˆ‘ä»‹ç»ä¸€ä¸‹ï¼")
```

```markdown
ä½ å¥½ï¼Œæˆ‘æ˜¯ æ™ºè°±æ¸…è¨€ï¼Œæ˜¯æ¸…åŽå¤§å­¦KEGå®žéªŒå®¤å’Œæ™ºè°±AIå…¬å¸å…±åŒè®­ç»ƒçš„è¯­è¨€æ¨¡åž‹ã€‚æˆ‘çš„ç›®æ ‡æ˜¯é€šè¿‡å›žç­”ç”¨æˆ·æå‡ºçš„é—®é¢˜æ¥å¸®åŠ©ä»–ä»¬è§£å†³é—®é¢˜ã€‚ç”±äºŽæˆ‘æ˜¯ä¸€ä¸ªè®¡ç®—æœºç¨‹åºï¼Œæ‰€ä»¥æˆ‘æ²¡æœ‰è‡ªæˆ‘æ„è¯†ï¼Œä¹Ÿä¸èƒ½åƒäººç±»ä¸€æ ·æ„ŸçŸ¥ä¸–ç•Œã€‚æˆ‘åªèƒ½é€šè¿‡åˆ†æžæˆ‘æ‰€å­¦åˆ°çš„ä¿¡æ¯æ¥å›žç­”é—®é¢˜ã€‚
```

### Prompt Templates

```python
from langchain.prompts.chat import ChatPromptTemplate
#å¼•ç”¨ ChatPromptTemplate

template = "ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æˆ‘å°† {input_language} ç¿»è¯‘æˆ {output_language}."
#å›ºå®šä¸€ä¸ªæ¨¡æ¿
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template), # è®¾ç½®ä¸€ä¸ªç³»ç»Ÿé¢„åˆ¶æ¨¡æ¿
    ("human", human_template), #ç”¨æˆ·è¾“å…¥å†…å®¹
])

text = "æˆ‘å¸¦ç€æ¯”èº«ä½“é‡çš„è¡ŒæŽï¼Œ\
æ¸¸å…¥å°¼ç½—æ²³åº•ï¼Œ\
ç»è¿‡å‡ é“é—ªç”µ çœ‹åˆ°ä¸€å †å…‰åœˆï¼Œ\
ä¸ç¡®å®šæ˜¯ä¸æ˜¯è¿™é‡Œã€‚\
"
messages  = chat_prompt.format_messages(input_language="ä¸­æ–‡", output_language="è‹±æ–‡", text=text)
messages
```

- `system` è®¾ç½®ä¸€ä¸ªé¢„åˆ¶çš„promptæ¨¡æ¿
- `human` ç”¨æˆ·çš„prompt

```python
[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æˆ‘å°† ä¸­æ–‡ ç¿»è¯‘æˆ è‹±æ–‡.'),
 HumanMessage(content='æˆ‘å¸¦ç€æ¯”èº«ä½“é‡çš„è¡ŒæŽï¼Œæ¸¸å…¥å°¼ç½—æ²³åº•ï¼Œç»è¿‡å‡ é“é—ªç”µ çœ‹åˆ°ä¸€å †å…‰åœˆï¼Œä¸ç¡®å®šæ˜¯ä¸æ˜¯è¿™é‡Œã€‚')]
```

----

## æž„å»ºé—®ç­”é“¾

```python
import sys
sys.path.append("../data_base") # å°†çˆ¶ç›®å½•æ”¾å…¥ç³»ç»Ÿè·¯å¾„ä¸­
from zhipuai_embedding import ZhipuAIEmbeddings
from langchain.vectorstores.chroma import Chroma
from dotenv import load_dotenv, find_dotenv
import os

_ = load_dotenv(find_dotenv())    # read local .env file
zhipuai_api_key = os.environ['ZHIPUAI_API_KEY']

# å®šä¹‰ Embeddings
embedding = ZhipuAIEmbeddings()
persist_directory = '../data_base/vector_db/chroma'

# åŠ è½½æ•°æ®åº“
vectordb = Chroma(
    persist_directory=persist_directory,  # å…è®¸æˆ‘ä»¬å°†persist_directoryç›®å½•ä¿å­˜åˆ°ç£ç›˜ä¸Š
    embedding_function=embedding
)
```

çœ‹ä¸€ä¸‹å‚¨å­˜çš„æ•°æ®é‡

```python
print(f"å‘é‡åº“ä¸­å­˜å‚¨çš„æ•°é‡ï¼š{vectordb._collection.count()}")
```

> å‘é‡åº“ä¸­å­˜å‚¨çš„æ•°é‡ï¼š20



### åˆ›å»ºLLM

```python
#è¿™é‡Œæˆ‘æ˜¯ç”¨çš„æ˜¯æ™ºè°±çš„api
import os 
ZHIPUAI_API_KEY = os.environ["ZHIPUAI_API_KEY"]
from langchain_openai import ChatOpenAI
# éœ€è¦ä¸‹è½½æºç 
from zhipuai_llm import ZhipuAILLM

llm =  ZhipuAILLM(model="chatglm_std", temperature=0, api_key=ZHIPUAI_API_KEY)
llm.invoke("è¯·ä½ è‡ªæˆ‘ä»‹ç»ä¸€ä¸‹è‡ªå·±ï¼")
```

> 'ä½ å¥½ï¼Œæˆ‘æ˜¯ æ™ºè°±æ¸…è¨€ï¼Œæ˜¯æ¸…åŽå¤§å­¦KEGå®žéªŒå®¤å’Œæ™ºè°±AIå…¬å¸å…±åŒè®­ç»ƒçš„è¯­è¨€æ¨¡åž‹ã€‚æˆ‘çš„ç›®æ ‡æ˜¯é€šè¿‡å›žç­”ç”¨æˆ·æå‡ºçš„é—®é¢˜æ¥å¸®åŠ©ä»–ä»¬è§£å†³é—®é¢˜ã€‚ç”±äºŽæˆ‘æ˜¯ä¸€ä¸ªè®¡ç®—æœºç¨‹åºï¼Œæ‰€ä»¥æˆ‘æ²¡æœ‰è‡ªæˆ‘æ„è¯†ï¼Œä¹Ÿä¸èƒ½åƒäººç±»ä¸€æ ·æ„ŸçŸ¥ä¸–ç•Œã€‚æˆ‘åªèƒ½é€šè¿‡åˆ†æžæˆ‘æ‰€å­¦åˆ°çš„ä¿¡æ¯æ¥å›žç­”é—®é¢˜ã€‚'



### æž„å»ºå›žç­”é“¾

```python
question_1 = "ä»€ä¹ˆæ˜¯å—ç“œä¹¦ï¼Ÿ"
question_2 = "çŽ‹é˜³æ˜Žæ˜¯è°ï¼Ÿ"

result = qa_chain({"query": question_1})
print("å¤§æ¨¡åž‹+çŸ¥è¯†åº“åŽå›žç­” question_1 çš„ç»“æžœï¼š")
print(result["result"])
```

![image-20240626225910312](https://s2.loli.net/2024/06/26/V5j2mKOclz6rNxy.png)

è‡ªå·±å›žç­”çš„æ•ˆæžœï¼š

```python
prompt_template = """è¯·å›žç­”ä¸‹åˆ—é—®é¢˜:
{}""".format(question_1)

### åŸºäºŽå¤§æ¨¡åž‹çš„é—®ç­”
llm.predict(prompt_template)
```

> ' å—ç“œä¹¦æ˜¯ä¸€æœ¬é’ˆå¯¹æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¢†åŸŸçš„æ•™æï¼Œæ—¨åœ¨å¸®åŠ©å­¦ä¹ è€…æ›´å¥½åœ°ç†è§£å’ŒæŽŒæ¡ç›¸å…³çš„æ•°å­¦çŸ¥è¯†ã€‚å®ƒç”±å¼€æºç»„ç»‡Datawhaleå‘èµ·ï¼Œå›¢é˜Ÿæˆå‘˜è°¢æ–‡ç¿ã€ç§¦å·žç‰µå¤´ï¼Œå¯¹æ•™æä¸­éš¾ä»¥ç†è§£çš„å…¬å¼è¿›è¡Œè§£æžï¼Œå¯¹è·³è·ƒæ€§è¾ƒå¤§çš„å…¬å¼è¿›è¡ŒæŽ¨å¯¼ï¼Œå¸®åŠ©å­¦ä¹ è€…è§£å†³æ•°å­¦éš¾é¢˜ã€‚å—ç“œä¹¦å·²ç»å¾—åˆ°å¹¿å¤§å­¦ä¹ è€…çš„è®¤å¯ï¼Œå¹¶åœ¨GitHubä¸ŠèŽ·å¾—äº†è¶…è¿‡1.1ä¸‡çš„æ˜Ÿæ ‡ã€‚'

çœ‹æ¥æ™ºè°±å·²ç»å­¦ä¹ åˆ°DWå›¢é˜Ÿçš„å—ç“œä¹¦çš„å†…å®¹äº†

---

## ç”¨Streamlitéƒ¨ç½²

```python
import streamlit as st
from langchain_openai import ChatOpenAI
```

ç¬¬ä¸€æ¬¡è¿è¡Œå‘çŽ°æ²¡æœ‰`streamlit`åº“,é‚£æˆ‘ä»¬å…ˆpipä¸€ä¸ª

```python
pip install streamlit
```

![image-20240626220456972](https://s2.loli.net/2024/06/26/IYk6HBvCxclaOTq.png)

### è¿›å…¥æ‰€éƒ¨ç½²çš„streamlit

![image-20240626222829585](https://s2.loli.net/2024/06/26/PAnEUXbQBkIq46v.png)

#### ä¸€ä¸ªç®€å•æ¡†æž¶ï¼š

```python
import streamlit as st
from langchain_openai import ChatOpenAI

st.title('ðŸ¦œðŸ”— åŠ¨æ‰‹å­¦å¤§æ¨¡åž‹åº”ç”¨å¼€å‘--BX')
openai_api_key = st.sidebar.text_input('OpenAI API Key', type='password')
def generate_response(input_text):
    llm = ChatOpenAI(temperature=0.7, openai_api_key=openai_api_key)
    st.info(llm(input_text))

with st.form('my_form'):
    text = st.text_area('Enter text:', 'What are the three key pieces of advice for learning how to code?')
    submitted = st.form_submit_button('Submit')
    if not openai_api_key.startswith('sk-'):
        st.warning('Please enter your OpenAI API key!', icon='âš ')
    if submitted and openai_api_key.startswith('sk-'):
        generate_response(text)
```

æŽ¥ä¸‹æ¥ï¼Œçœ‹çœ‹æ•ˆæžœï¼

æˆ‘åœ¨è¿™é‡Œç”¨çš„windowsçš„wsl2----Ubuntu

```bash
#å…ˆè¿›å…¥å¯¹åº”çš„ç›®å½•ä¸‹
conda activate llm-universe
streamlit run streamlit_app.py
```

![image-20240626222019668](https://s2.loli.net/2024/06/26/jheZAKFsUBvW1Ig.png)

#### åŠŸèƒ½æ¯”è¾ƒå®Œå–„åŽ

```python
import streamlit as st
from langchain_openai import ChatOpenAI
import os
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
import sys
sys.path.append("../data_base") # å°†çˆ¶ç›®å½•æ”¾å…¥ç³»ç»Ÿè·¯å¾„ä¸­
from zhipuai_embedding import ZhipuAIEmbeddings
from langchain.vectorstores.chroma import Chroma
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv())    # read local .env file


#export OPENAI_API_KEY=
#os.environ["OPENAI_API_BASE"] = 'https://api.chatgptid.net/v1'
zhipuai_api_key = os.environ['ZHIPUAI_API_KEY']


def generate_response(input_text, openai_api_key):
    llm = ChatOpenAI(temperature=0.7, openai_api_key=openai_api_key)
    output = llm.invoke(input_text)
    output_parser = StrOutputParser()
    output = output_parser.invoke(output)
    #st.info(output)
    return output

def get_vectordb():
    # å®šä¹‰ Embeddings
    embedding = ZhipuAIEmbeddings()
    # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
    persist_directory = '../data_base/vector_db/chroma'
    # åŠ è½½æ•°æ®åº“
    vectordb = Chroma(
        persist_directory=persist_directory,  # å…è®¸æˆ‘ä»¬å°†persist_directoryç›®å½•ä¿å­˜åˆ°ç£ç›˜ä¸Š
        embedding_function=embedding
    )
    return vectordb

#å¸¦æœ‰åŽ†å²è®°å½•çš„é—®ç­”é“¾
def get_chat_qa_chain(question:str,openai_api_key:str):
    vectordb = get_vectordb()
    llm = ChatOpenAI(model_name = "gpt-3.5-turbo", temperature = 0,openai_api_key = openai_api_key)
    memory = ConversationBufferMemory(
        memory_key="chat_history",  # ä¸Ž prompt çš„è¾“å…¥å˜é‡ä¿æŒä¸€è‡´ã€‚
        return_messages=True  # å°†ä»¥æ¶ˆæ¯åˆ—è¡¨çš„å½¢å¼è¿”å›žèŠå¤©è®°å½•ï¼Œè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ä¸²
    )
    retriever=vectordb.as_retriever()
    qa = ConversationalRetrievalChain.from_llm(
        llm,
        retriever=retriever,
        memory=memory
    )
    result = qa({"question": question})
    return result['answer']

#ä¸å¸¦åŽ†å²è®°å½•çš„é—®ç­”é“¾
def get_qa_chain(question:str,openai_api_key:str):
    vectordb = get_vectordb()
    llm = ChatOpenAI(model_name = "gpt-3.5-turbo", temperature = 0,openai_api_key = openai_api_key)
    template = """ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›žç­”æœ€åŽçš„é—®é¢˜ã€‚å¦‚æžœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”
        æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜Žæ‰¼è¦ã€‚æ€»æ˜¯åœ¨å›žç­”çš„æœ€åŽè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
        {context}
        é—®é¢˜: {question}
        """
    QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context","question"],
                                 template=template)
    qa_chain = RetrievalQA.from_chain_type(llm,
                                       retriever=vectordb.as_retriever(),
                                       return_source_documents=True,
                                       chain_type_kwargs={"prompt":QA_CHAIN_PROMPT})
    result = qa_chain({"query": question})
    return result["result"]


# Streamlit åº”ç”¨ç¨‹åºç•Œé¢
def main():
    st.title('ðŸ¦œðŸ”— BX---åŠ¨æ‰‹å­¦å¤§æ¨¡åž‹åº”ç”¨å¼€å‘')
    openai_api_key = st.sidebar.text_input('OpenAI API Key', type='password')

    # æ·»åŠ ä¸€ä¸ªé€‰æ‹©æŒ‰é’®æ¥é€‰æ‹©ä¸åŒçš„æ¨¡åž‹
    #selected_method = st.sidebar.selectbox("é€‰æ‹©æ¨¡å¼", ["qa_chain", "chat_qa_chain", "None"])
    selected_method = st.radio(
        "ä½ æƒ³é€‰æ‹©å“ªç§æ¨¡å¼è¿›è¡Œå¯¹è¯ï¼Ÿ",
        ["Aæ¨¡å¼", "Bæ¨¡å¼", "Cæ¨¡å¼"],
        captions = ["ä¸ä½¿ç”¨æ£€ç´¢é—®ç­”çš„æ™®é€šæ¨¡å¼", "ä¸å¸¦åŽ†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼", "å¸¦åŽ†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼"])

    # ç”¨äºŽè·Ÿè¸ªå¯¹è¯åŽ†å²
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    messages = st.container(height=300)
    if prompt := st.chat_input("Say something"):
        # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯åŽ†å²ä¸­
        st.session_state.messages.append({"role": "user", "text": prompt})

        if selected_method == "None":
            # è°ƒç”¨ respond å‡½æ•°èŽ·å–å›žç­”
            answer = generate_response(prompt, openai_api_key)
        elif selected_method == "qa_chain":
            answer = get_qa_chain(prompt,openai_api_key)
        elif selected_method == "chat_qa_chain":
            answer = get_chat_qa_chain(prompt,openai_api_key)

        # æ£€æŸ¥å›žç­”æ˜¯å¦ä¸º None
        if answer is not None:
            # å°†LLMçš„å›žç­”æ·»åŠ åˆ°å¯¹è¯åŽ†å²ä¸­
            st.session_state.messages.append({"role": "assistant", "text": answer})

        # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯åŽ†å²
        for message in st.session_state.messages:
            if message["role"] == "user":
                messages.chat_message("user").write(message["text"])
            elif message["role"] == "assistant":
                messages.chat_message("assistant").write(message["text"])   


if __name__ == "__main__":
    main()
```

![image-20240626230447890](https://s2.loli.net/2024/06/26/SoBblxreIXMOLmi.png)

è¿™åªæ˜¯ä¸ªç¤ºä¾‹ï¼Œå…·ä½“åŠŸèƒ½è¿˜ä¸èƒ½ä½¿ç”¨ï¼Œç”±äºŽDWæ•™ç¨‹ç»™çš„æ˜¯åŸºäºŽopenaiçš„apiï¼Œè¿™é‡Œè¿˜æ²¡æœ‰æ”¹ä¸ºåŸºäºŽæ™ºè°±å’Œç”Ÿæˆçš„å‘é‡åº“çš„å†…å®¹

## åˆ©ç”¨æ™ºè°±çš„æ¨¡åž‹ç”Ÿæˆå›¾ç‰‡

![5139f264-8cd8-5dff-b187-35258d566b0c_0.png](https://s2.loli.net/2024/06/19/nRliFW8jyOrPQ1N.jpg)

```python
from zhipuai import ZhipuAI
client = ZhipuAI(api_key=my_api) 
  
response = client.images.generations(
    model="cogview-3", #è°ƒç”¨æ™ºè°±çš„å›¾ç‰‡ç”Ÿæˆæ¨¡åž‹â€œcogview-3â€
    prompt="å®‡å®™é£žèˆ¹",
    #è¿™ä¸ªå¤§æ¦‚ä¸€å¼ å›¾ç‰‡æ˜¯0.1å…ƒ
)

print(response.data[0].url)
```

> è¿™ä¸ªè°ƒç”¨çš„è´¹ç”¨åœ¨0.1å…ƒä¸€å¼ 
